{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cbc0989-69d6-434c-ba5c-76df0d16dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adopted from:\n",
    "# https://towardsdatascience.com/double-deep-q-networks-905dd8325412\n",
    "# https://github.com/cyoon1729/deep-Q-networks/blob/master/doubleDQN/ddqn.py\n",
    "# https://towardsdatascience.com/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb\n",
    "\n",
    "# https://github.com/XinJingHao/DQN-DDQN-Pytorch/blob/main/DQN.py\n",
    "\n",
    "# used gpt-3 as well for skeleton experimentation code\n",
    "\n",
    "# https://goodboychan.github.io/python/reinforcement_learning/pytorch/udacity/2021/05/07/DQN-LunarLander.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c5ad7f-9464-4495-910a-fdd5c1a7626e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42d34eb1-3522-403c-b8f8-8de881fc9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adopted from\n",
    "# https://github.com/chengxi600/RLStuff/blob/master/Policy%20Optimization%20Algorithms/PPO_Discrete.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da817ffb-fa10-4723-8f5a-684022ab8290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import seaborn as sns\n",
    "import gym\n",
    "import random\n",
    "\n",
    "from collections import deque "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2138a3-fd22-4b45-9cfb-46b5955adeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tqdm import tqdm_notebook\n",
    "#from collections import deque\n",
    "#from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea5bb0c-b5a5-4dec-b153-6e6c282f15a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        '''\n",
    "        Args:\n",
    "        - obs_space (int): observation space\n",
    "        - action_space (int): action space\n",
    "        \n",
    "        '''\n",
    "        super(ActorCriticNetwork, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "                            nn.Linear(in_dim, hidden_dim),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(hidden_dim, hidden_dim),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(hidden_dim, out_dim),\n",
    "                            nn.Softmax(dim=1))\n",
    "\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(in_dim, hidden_dim),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(hidden_dim, hidden_dim),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(hidden_dim, 1))\n",
    "        \n",
    "    def forward(self):\n",
    "        ''' Not implemented since we call the individual actor and critc networks for forward pass\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        ''' Selects an action given current state\n",
    "        Args:\n",
    "        - network (Torch NN): network to process state\n",
    "        - state (Array): Array of action space in an environment\n",
    "\n",
    "        Return:\n",
    "        - (int): action that is selected\n",
    "        - (float): log probability of selecting that action given state and network\n",
    "        '''\n",
    "\n",
    "        # Setup state\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "\n",
    "        # Action probabilities\n",
    "        action_probs = self.actor(state)\n",
    "\n",
    "        # Sample an action using the probability distribution\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        # Return action\n",
    "        return action.item(), m.log_prob(action)\n",
    "    \n",
    "    def evaluate_action(self, states, actions):\n",
    "        ''' Get log probability and entropy of an action taken in given state\n",
    "        Args:\n",
    "        - states (Array): array of states to be evaluated\n",
    "        - actions (Array): array of actions to be evaluated\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # Convert state to float tensor, add 1 dimension, allocate tensor on device\n",
    "        states = torch.stack([torch.from_numpy(state).float().unsqueeze(0) for state in states]).squeeze(1)\n",
    "\n",
    "        # Use network to predict action probabilities\n",
    "        action_probs = self.actor(states)\n",
    "\n",
    "        # Get probability distribution\n",
    "        m = Categorical(action_probs)\n",
    "\n",
    "        #return log_prob and entropy\n",
    "        return m.log_prob(torch.Tensor(actions)), m.entropy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69b223c4-7e1f-45ec-94a2-f2464b72a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proximal Policy Optimization\n",
    "class PPO_policy():\n",
    "    \n",
    "    def __init__(self, γ, ϵ, β, δ, c1, c2, k_epoch, obs_space, action_space, α_θ, αv, hidden_dim=64):\n",
    "        '''\n",
    "        Args:\n",
    "        - γ (float): discount factor\n",
    "        - ϵ (float): soft surrogate objective constraint\n",
    "        - β (float): KL (Kullback–Leibler) penalty \n",
    "        - δ (float): KL divergence adaptive target\n",
    "        - c1 (float): value loss weight\n",
    "        - c2 (float): entropy weight\n",
    "        - k_epoch (int): number of epochs to optimize\n",
    "        - obs_space (int): observation space\n",
    "        - action_space (int): action space\n",
    "        - α_θ (float): actor learning rate\n",
    "        - αv (float): critic learning rate\n",
    "        \n",
    "        '''\n",
    "        self.γ = γ\n",
    "        self.ϵ = ϵ\n",
    "        self.β = β\n",
    "        self.δ = δ\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.k_epoch = k_epoch\n",
    "        self.actor_critic = ActorCriticNetwork(obs_space, hidden_dim, action_space)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.actor_critic.actor.parameters(), 'lr': α_θ},\n",
    "            {'params': self.actor_critic.critic.parameters(), 'lr': αv}\n",
    "        ])\n",
    "        \n",
    "        #buffer to store current batch\n",
    "        self.batch = []\n",
    "\n",
    "        self.loss_func = nn.MSELoss()\n",
    "    \n",
    "    def process_rewards(self, rewards, terminals):\n",
    "        ''' Converts our rewards history into cumulative discounted rewards\n",
    "        Args:\n",
    "        - rewards (Array): array of rewards \n",
    "\n",
    "        Returns:\n",
    "        - G (Array): array of cumulative discounted rewards\n",
    "        '''\n",
    "        #Calculate Gt (cumulative discounted rewards)\n",
    "        G = []\n",
    "\n",
    "        #track cumulative reward\n",
    "        total_r = 0\n",
    "\n",
    "        #iterate rewards from Gt to G0\n",
    "        for r, done in zip(reversed(rewards), reversed(terminals)):\n",
    "\n",
    "            #Base case: G(T) = r(T)\n",
    "            #Recursive: G(t) = r(t) + G(t+1)^DISCOUNT\n",
    "            total_r = r + total_r * self.γ\n",
    "\n",
    "            #no future rewards if current step is terminal\n",
    "            if done:\n",
    "                total_r = r\n",
    "\n",
    "            #add to front of G\n",
    "            G.insert(0, total_r)\n",
    "\n",
    "        #whitening rewards\n",
    "        G = torch.tensor(G)\n",
    "        G = (G - G.mean())/G.std()\n",
    "\n",
    "        return G\n",
    "    \n",
    "    def kl_divergence(self, old_lps, new_lps):\n",
    "        ''' Calculate distance between two distributions with KL divergence\n",
    "        Args:\n",
    "        - old_lps (Array): array of old policy log probabilities\n",
    "        - new_lps (Array): array of new policy log probabilities\n",
    "        '''\n",
    "        \n",
    "        #track kl divergence\n",
    "        total = 0\n",
    "        \n",
    "        #sum up divergence for all actions\n",
    "        for old_lp, new_lp in zip(old_lps, new_lps):\n",
    "            \n",
    "            #same as old_lp * log(old_prob/new_prob) cuz of log rules\n",
    "            total += old_lp * (old_lp - new_lp)\n",
    "\n",
    "        return total\n",
    "    \n",
    "    \n",
    "    def penalty_update(self):\n",
    "        ''' Update policy using surrogate objective with adaptive KL penalty\n",
    "        '''\n",
    "        \n",
    "        #get items from current batch\n",
    "        states = [sample[0] for sample in self.batch]\n",
    "        actions = [sample[1] for sample in self.batch]\n",
    "        rewards = [sample[2] for sample in self.batch]\n",
    "        old_lps = [sample[3] for sample in self.batch]\n",
    "        terminals = [sample[4] for sample in self.batch]\n",
    "\n",
    "        #calculate cumulative discounted rewards\n",
    "        Gt = self.process_rewards(rewards, terminals)\n",
    "\n",
    "        #track divergence\n",
    "        divergence = 0\n",
    "\n",
    "        #perform k-epoch update\n",
    "        for epoch in range(self.k_epoch):\n",
    "\n",
    "            #get ratio\n",
    "            new_lps, entropies = self.actor_critic.evaluate_action(states, actions)\n",
    "            #same as new_prob / old_prob\n",
    "            ratios = torch.exp(new_lps - torch.Tensor(old_lps))\n",
    "\n",
    "            #compute advantages\n",
    "            states_tensor = torch.stack([torch.from_numpy(state).float().unsqueeze(0) for state in states]).squeeze(1)\n",
    "            vals = self.actor_critic.critic(states_tensor).squeeze(1).detach()\n",
    "            advantages = Gt - vals\n",
    "\n",
    "            #get loss with adaptive kl penalty\n",
    "            divergence = self.kl_divergence(old_lps, new_lps).detach()\n",
    "            loss = -ratios * advantages + self.β * divergence\n",
    "\n",
    "            #SGD via Adam\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        #update adaptive penalty\n",
    "        if divergence >= 1.5 * self.δ:\n",
    "            self.β *= 2\n",
    "        elif divergence <= self.δ / 1.5:\n",
    "            self.β /= 2\n",
    "        \n",
    "        #clear batch buffer\n",
    "        self.batch = []\n",
    "            \n",
    "    def clipped_update(self):\n",
    "        ''' Update policy using clipped surrogate objective\n",
    "        '''\n",
    "        #get items from trajectory\n",
    "        states = [sample[0] for sample in self.batch]\n",
    "        actions = [sample[1] for sample in self.batch]\n",
    "        rewards = [sample[2] for sample in self.batch]\n",
    "        old_lps = [sample[3] for sample in self.batch]\n",
    "        terminals = [sample[4] for sample in self.batch]\n",
    "\n",
    "        #calculate cumulative discounted rewards\n",
    "        Gt = self.process_rewards(rewards, terminals)\n",
    "\n",
    "        #perform k-epoch update\n",
    "        for epoch in range(self.k_epoch):\n",
    "\n",
    "            #get ratio\n",
    "            new_lps, entropies = self.actor_critic.evaluate_action(states, actions)\n",
    "\n",
    "            ratios = torch.exp(new_lps - torch.Tensor(old_lps))\n",
    "\n",
    "            #compute advantages\n",
    "            states_tensor = torch.stack([torch.from_numpy(state).float().unsqueeze(0) for state in states]).squeeze(1)\n",
    "            vals = self.actor_critic.critic(states_tensor).squeeze(1).detach()\n",
    "            advantages = Gt - vals\n",
    "\n",
    "            #clip surrogate objective\n",
    "            surrogate1 = torch.clamp(ratios, min=1 - self.ϵ, max=1 + self.ϵ) * advantages\n",
    "            surrogate2 = ratios * advantages\n",
    "\n",
    "            #loss, flip signs since this is gradient descent\n",
    "            loss =  -torch.min(surrogate1, surrogate2) + self.c1 * self.loss_func(Gt, vals) - self.c2 * entropies\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        #clear batch buffer\n",
    "        self.batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8fd890e-c450-4f1c-9265-a73b18a14e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make environment\n",
    "#env = gym.make('CartPole-v1')\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "#seeds\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(2)\n",
    "\n",
    "#environment parameters\n",
    "obs_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c824b6be-4fae-4085-8b4a-3088f6e5880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment Hyperparameters\n",
    "\n",
    "#CartPole hyperparameters\n",
    "ppo_policy = PPO_policy(γ=0.99, ϵ=0.2, β=1, δ=0.01, c1=0.5, c2=0.01, k_epoch=40, \n",
    "                        obs_space=obs_space, action_space=action_space, α_θ = 1e-3, αv = 1e-3, hidden_dim=64)\n",
    "                        #obs_space=obs_space, action_space=action_space, α_θ = 0.0003, αv = 0.001, hidden_dim=64)\n",
    "\n",
    "#number of steps to train\n",
    "TRAIN_STEPS = 100000\n",
    "\n",
    "#max steps per episode\n",
    "MAX_STEPS = 400\n",
    "\n",
    "#batch training size\n",
    "BATCH_SIZE = 1600\n",
    "\n",
    "#solved environment score\n",
    "SOLVED_SCORE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0833af02-f0f5-497a-9d7e-30205bb5d345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2500\tAverage Score: -177.43\n",
      "Step 5000\tAverage Score: -151.54\n",
      "Step 7500\tAverage Score: -135.88\n",
      "Step 10000\tAverage Score: -128.90\n",
      "Step 12500\tAverage Score: -104.55\n",
      "Step 15000\tAverage Score: -91.47\n",
      "Step 17500\tAverage Score: -88.67\n",
      "Step 20000\tAverage Score: -74.06\n",
      "Step 22500\tAverage Score: -60.94\n",
      "Step 25000\tAverage Score: -45.83\n",
      "Step 27500\tAverage Score: -41.77\n",
      "Step 30000\tAverage Score: -36.34\n",
      "Step 32500\tAverage Score: -32.85\n",
      "Step 35000\tAverage Score: -28.06\n",
      "Step 37500\tAverage Score: -22.83\n",
      "Step 40000\tAverage Score: -12.00\n",
      "Step 42500\tAverage Score: -2.22\n",
      "Step 45000\tAverage Score: -3.49\n",
      "Step 47500\tAverage Score: 2.37\n",
      "Step 50000\tAverage Score: 5.44\n",
      "Step 52500\tAverage Score: 8.85\n",
      "Step 55000\tAverage Score: 11.39\n",
      "Step 57500\tAverage Score: 13.38\n",
      "Step 60000\tAverage Score: 20.33\n",
      "Step 62500\tAverage Score: 23.97\n",
      "Step 65000\tAverage Score: 29.15\n",
      "Step 67500\tAverage Score: 34.44\n",
      "Step 70000\tAverage Score: 34.34\n",
      "Step 72500\tAverage Score: 34.31\n",
      "Step 75000\tAverage Score: 32.49\n",
      "Step 77500\tAverage Score: 32.12\n",
      "Step 80000\tAverage Score: 30.13\n",
      "Step 82500\tAverage Score: 33.33\n",
      "Step 85000\tAverage Score: 33.23\n",
      "Step 87500\tAverage Score: 33.26\n",
      "Step 90000\tAverage Score: 34.81\n",
      "Step 92500\tAverage Score: 30.16\n",
      "Step 95000\tAverage Score: 33.04\n",
      "Step 97500\tAverage Score: 31.46\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "scores = []\n",
    "scores_window = deque(maxlen=100)\n",
    "state, _ = env.reset()\n",
    "curr_step = 0\n",
    "score = 0\n",
    "\n",
    "# Train\n",
    "for step in range(1, TRAIN_STEPS):\n",
    "    curr_step += 1\n",
    "    action, lp = ppo_policy.actor_critic.select_action(state)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    score += reward\n",
    "    ppo_policy.batch.append([state, action, reward, lp, done])\n",
    "\n",
    "    # Optimize surrogate objective\n",
    "    if step % BATCH_SIZE == 0:\n",
    "        ppo_policy.clipped_update()\n",
    "    \n",
    "    if step % 2500 == 0:\n",
    "        print('\\rStep {}\\tAverage Score: {:.2f}'.format(step, np.mean(scores_window)))\n",
    "    if np.mean(scores_window)>=200.0:\n",
    "        print('\\nEnvironment solved in {:d} steps!\\tAverage Score: {:.2f}'.format(step-2500, np.mean(scores_window)))\n",
    "        #torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "        break\n",
    "\n",
    "    # End episode\n",
    "    if done or curr_step >= MAX_STEPS:\n",
    "        state, _ = env.reset()\n",
    "        curr_step = 0\n",
    "        scores.append(score)\n",
    "        scores_window.append(score)\n",
    "        score = 0\n",
    "        continue\n",
    "        \n",
    "    # Check if solved environment, early stopping\n",
    "    if len(recent_scores) >= 100 and np.array(recent_scores).mean() >= SOLVED_SCORE:\n",
    "        break\n",
    "\n",
    "    # Set state\n",
    "    state = next_state   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7369854d-8eab-4660-beac-79ce488512bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CartPole hyperparameters\n",
    "ppo_policy = PPO_policy(γ=0.99, ϵ=0.2, β=1, δ=0.01, c1=0.5, c2=0.01, k_epoch=40, \n",
    "                        obs_space=obs_space, action_space=action_space, α_θ = 1e-3, αv = 1e-3, hidden_dim=64)\n",
    "                        #obs_space=obs_space, action_space=action_space, α_θ = 0.0003, αv = 0.001, hidden_dim=64)\n",
    "\n",
    "#number of steps to train\n",
    "max_episodes = 1000\n",
    "\n",
    "#max steps per episode\n",
    "max_steps = 500\n",
    "\n",
    "#batch training size\n",
    "#BATCH_SIZE = 1600\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "#solved environment score\n",
    "SOLVED_SCORE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0cc97328-662b-45d3-b27a-f2defc8458de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tAverage Score: -143.16\n",
      "Episode 100\tAverage Score: -192.48\n",
      "Episode 200\tAverage Score: -203.06\n",
      "Episode 300\tAverage Score: -171.09\n",
      "Episode 400\tAverage Score: -150.24\n",
      "Episode 500\tAverage Score: -192.54\n",
      "Episode 600\tAverage Score: -203.55\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m     11\u001b[0m     curr_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 12\u001b[0m     action, lp \u001b[38;5;241m=\u001b[39m \u001b[43mppo_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_critic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     14\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m, in \u001b[0;36mActorCriticNetwork.select_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     48\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Sample an action using the probability distribution\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m action \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Return action\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/categorical.py:70\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     67\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[1;32m     69\u001b[0m )\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/distribution.py:66\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[1;32m     65\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[0;32m---> 66\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/constraints.py:440\u001b[0m, in \u001b[0;36m_Simplex.check\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m ((value\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mabs() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "scores = []\n",
    "scores_window = deque(maxlen=100)\n",
    "state, _ = env.reset()\n",
    "curr_step = 0\n",
    "score = 0\n",
    "\n",
    "# Train\n",
    "for episode in range(max_episodes):\n",
    "    for step in range(max_steps):\n",
    "        curr_step += 1\n",
    "        action, lp = ppo_policy.actor_critic.select_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        score += reward\n",
    "        ppo_policy.batch.append([state, action, reward, lp, done])\n",
    "    \n",
    "        # Optimize surrogate objective\n",
    "        if step >= BATCH_SIZE:\n",
    "            ppo_policy.clipped_update()\n",
    "\n",
    "        # End episode\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    state, _ = env.reset()\n",
    "    curr_step = 0\n",
    "    scores.append(score)\n",
    "    scores_window.append(score)\n",
    "    score = 0\n",
    "            \n",
    "    if episode % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_window)))\n",
    "    if np.mean(scores_window)>=200.0:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode-100, np.mean(scores_window)))\n",
    "        #torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "        break\n",
    "\n",
    "    # Check if solved environment, early stopping\n",
    "    if len(recent_scores) >= 100 and np.array(recent_scores).mean() >= SOLVED_SCORE:\n",
    "        break\n",
    "\n",
    "    # Set state\n",
    "    state = next_state   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b2197-f264-43b9-a6df-1a555b9de13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
